{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retraining MobileNet V2 for Person Detection and ESP32 Compatibility\n",
    "## Retraining MobileNet V2 for Person Detection\n",
    "+ Utilize a pretrained MobileNet V2 model designed to detect 1000 classes.\n",
    "+ Retrain the model to detect the presence of a person in an image.\n",
    "## Modifying the Model for ESP32 Compatibility\n",
    "+ Convert the model input to accept flat array images.\n",
    "+ Resize images to match the model's input size.\n",
    "+Quantize the model for efficient execution on the ESP32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_CHANNELS = 3\n",
    "\n",
    "# input image (ie image from esp 32 looks size)\n",
    "IMAGE_HEIGHT = 240\n",
    "IMAGE_WIDTH = 240\n",
    "IMAGE_SHAPE = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS)\n",
    "\n",
    "# pretrained MobileNetV2 model image size\n",
    "BASE_MODEL_IMAGE_HEIGHT = 96\n",
    "BASE_MODEL_IMAGE_WIDTH = 96\n",
    "BASE_MODEL_IMAGE_SHAPE = (BASE_MODEL_IMAGE_HEIGHT, BASE_MODEL_IMAGE_WIDTH, IMAGE_CHANNELS)\n",
    "\n",
    "# final model input shape (flat)\n",
    "MODEL_INPUT_SHAPE = (IMAGE_CHANNELS * IMAGE_HEIGHT * IMAGE_WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set file location\n",
    "data_dir = 'G:\\ml_datasets\\coco'\n",
    "\n",
    "# Load the dataset with the specified data directory\n",
    "ds_train, ds_validation = tfds.load('coco/2017', split=['train','validation'], data_dir=data_dir)\n",
    "\n",
    "# Define the label for 'person' in the dataset\n",
    "PERSON_LABEL = 1  # Assuming 'person' has label 1, adjust this according to your dataset\n",
    "\n",
    "# Function to check if a person is in the image\n",
    "def has_person(example):\n",
    "    objects = example['objects']['label']\n",
    "    person_present = tf.reduce_any(tf.equal(objects, PERSON_LABEL))\n",
    "    example['person_present'] = tf.cast(person_present, tf.int64)\n",
    "    return example\n",
    "# Function to check if a person is in the image\n",
    "def format_image(example):\n",
    "  image= tf.image.resize(image, [IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "  #image=tf.dtypes.cast(image, tf.float32)/255\n",
    "  return tf.reshape(image,[IMAGE_CHANNELS * IMAGE_HEIGHT * IMAGE_WIDTH]), label\n",
    "    \n",
    "\n",
    "# Apply the mapping function to the dataset\n",
    "ds_train = ds_train.map(has_person)\n",
    "ds_validation = ds_validation.map(format_image)\n",
    "ds_train = ds_train.map(format_image)\n",
    "ds_validation = ds_validation.map(format_image)\n",
    "# Build your input pipeline\n",
    "ds_train = ds_train.shuffle(1024).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Iterate through the dataset and print an example\n",
    "for example in ds_train.take(1):\n",
    "    image, label, person_present = example[\"image\"], example[\"label\"], example[\"person_present\"]\n",
    "    print(f\"Person present: {person_present.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting a base model\n",
    "It is often easier to take an exist model and retrain for more specific tasks\n",
    "This example use a pretrained version of mobilenetv2 as a feature extrature\n",
    "The confulation filters are able to generate many feature from the image the could be things like red vertical lines, yello dots etc\n",
    "We will add new layers to take the input and predrict our clases\n",
    "\n",
    "Note not all parametes this kera function have pretrianed weights.\n",
    "## tranfer learing https://www.tensorflow.org/tutorials/images/transfer_learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model =tf.keras.applications.MobileNetV2(\n",
    "        input_shape=BASE_MODEL_IMAGE_SHAPE,\n",
    "        alpha=0.35,\n",
    "        include_top=False,# the top is the last layer of the model (the classifier) we don't want to include it we will train our own final layer\n",
    "        weights=\"imagenet\",\n",
    "        input_tensor=None,\n",
    "        pooling=None,\n",
    "        classes=1000,\n",
    "        classifier_activation=None,\n",
    "    )\n",
    "# Freeze the base model because we only want to train the new classifier\n",
    "# We don't want to train the base model because it has already learned many features\n",
    "# training new features is more difficult than training the classifier and would require more data\n",
    "base_model.trainable = False\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO might be better to define the base model with the flat input to make batching work\n",
    "# this is not goinh to work as is it will provide flat input to the shaped input of the base mmodel\n",
    "\n",
    "# need to know batch size for some layers?\n",
    "image_batch, label_batch = next(iter(ds_train))\n",
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)\n",
    "\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "print(feature_batch_average.shape)\n",
    "\n",
    "# new bnnary classifier layer\n",
    "prediction_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "prediction_batch = prediction_layer(feature_batch_average)\n",
    "print(prediction_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.RandomFlip('horizontal'),\n",
    "  tf.keras.layers.RandomRotation(0.2),\n",
    "])\n",
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_input = tf.keras.layers.Input(shape=(IMAGE_CHANNELS * IMAGE_HEIGHT * IMAGE_WIDTH))\n",
    "reshape = tf.keras.layers.Reshape(IMAGE_SHAPE)(flat_input)\n",
    "resize = tf.keras.layers.Resizing(\n",
    "    BASE_MODEL_IMAGE_HEIGHT,\n",
    "    BASE_MODEL_IMAGE_WIDTH,\n",
    "    interpolation='nearest',#\"bilinear\", \"nearest\" are compatible with tf micro\n",
    "    crop_to_aspect_ratio=True\n",
    "    )(reshape)\n",
    "x = data_augmentation(resize)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = prediction_layer(x)\n",
    "model = tf.keras.Model(flat_input, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.5, name='accuracy')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epochs = 10\n",
    "\n",
    "loss0, accuracy0 = model.evaluate(ds_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(ds_train,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=ds_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "windows-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
